
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Machine Learning Things">
      
      
      
        <link rel="canonical" href="https://gmihaila.github.io/ml_things/tutorial_notebooks/pytorchtext_bucketiterator/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>PyTorchText BucketIterator - Machine Learning Things</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="black">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#better-batches-with-pytorchtext-bucketiterator" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Things" class="md-header__button md-logo" aria-label="Machine Learning Things" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M8.75 8a.75.75 0 0 0-.75.75v6.5c0 .414.336.75.75.75h6.5a.75.75 0 0 0 .75-.75v-6.5a.75.75 0 0 0-.75-.75h-6.5zm.75 6.5v-5h5v5h-5z"/><path fill-rule="evenodd" d="M15.25 1a.75.75 0 0 1 .75.75V4h2.25c.966 0 1.75.784 1.75 1.75V8h2.25a.75.75 0 0 1 0 1.5H20v5h2.25a.75.75 0 0 1 0 1.5H20v2.25A1.75 1.75 0 0 1 18.25 20H16v2.25a.75.75 0 0 1-1.5 0V20h-5v2.25a.75.75 0 0 1-1.5 0V20H5.75A1.75 1.75 0 0 1 4 18.25V16H1.75a.75.75 0 0 1 0-1.5H4v-5H1.75a.75.75 0 0 1 0-1.5H4V5.75C4 4.784 4.784 4 5.75 4H8V1.75a.75.75 0 0 1 1.5 0V4h5V1.75a.75.75 0 0 1 .75-.75zm3 17.5a.25.25 0 0 0 .25-.25V5.75a.25.25 0 0 0-.25-.25H5.75a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Things
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PyTorchText BucketIterator
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/gmihaila/ml_things" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Things" class="md-nav__button md-logo" aria-label="Machine Learning Things" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M8.75 8a.75.75 0 0 0-.75.75v6.5c0 .414.336.75.75.75h6.5a.75.75 0 0 0 .75-.75v-6.5a.75.75 0 0 0-.75-.75h-6.5zm.75 6.5v-5h5v5h-5z"/><path fill-rule="evenodd" d="M15.25 1a.75.75 0 0 1 .75.75V4h2.25c.966 0 1.75.784 1.75 1.75V8h2.25a.75.75 0 0 1 0 1.5H20v5h2.25a.75.75 0 0 1 0 1.5H20v2.25A1.75 1.75 0 0 1 18.25 20H16v2.25a.75.75 0 0 1-1.5 0V20h-5v2.25a.75.75 0 0 1-1.5 0V20H5.75A1.75 1.75 0 0 1 4 18.25V16H1.75a.75.75 0 0 1 0-1.5H4v-5H1.75a.75.75 0 0 1 0-1.5H4V5.75C4 4.784 4.784 4 5.75 4H8V1.75a.75.75 0 0 1 1.5 0V4h5V1.75a.75.75 0 0 1 .75-.75zm3 17.5a.25.25 0 0 0 .25-.25V5.75a.25.25 0 0 0-.25-.25H5.75a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5z"/></svg>

    </a>
    Machine Learning Things
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/gmihaila/ml_things" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../useful/useful/" class="md-nav__link">
        Useful Code
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Tutorial Notebooks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorial Notebooks" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tutorial Notebooks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../bert_inner_workings/" class="md-nav__link">
        Bert Inner Workings
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          PyTorchText BucketIterator
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        PyTorchText BucketIterator
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-to-use-pytorchtext-bucketiterator-to-sort-text-data-for-better-batching" class="md-nav__link">
    How to use PyTorchText BucketIterator to sort text data for better batching.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-should-i-know-for-this-notebook" class="md-nav__link">
    What should I know for this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-use-this-notebook" class="md-nav__link">
    How to use this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding" class="md-nav__link">
    Coding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#downloads" class="md-nav__link">
    Downloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installs" class="md-nav__link">
    Installs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-pytorch-dataset" class="md-nav__link">
    Using PyTorch Dataset
  </a>
  
    <nav class="md-nav" aria-label="Using PyTorch Dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset-class" class="md-nav__link">
    Dataset Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train---validation-datasets" class="md-nav__link">
    Train - Validation Datasets
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-dataloader" class="md-nav__link">
    PyTorch DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorchtext-bucket-iterator-dataloader" class="md-nav__link">
    PyTorchText Bucket Iterator Dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare-dataloaders" class="md-nav__link">
    Compare DataLoaders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train-loop-examples" class="md-nav__link">
    Train Loop Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-pytorchtext-tabulardataset" class="md-nav__link">
    Using PyTorchText TabularDataset
  </a>
  
    <nav class="md-nav" aria-label="Using PyTorchText TabularDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-to-files" class="md-nav__link">
    Data to Files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tabulardataset" class="md-nav__link">
    TabularDataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorchtext-bucket-iterator-dataloader_1" class="md-nav__link">
    PyTorchText Bucket Iterator Dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare-dataloaders_1" class="md-nav__link">
    Compare DataLoaders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train-loop-examples_1" class="md-nav__link">
    Train Loop Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-note" class="md-nav__link">
    Final Note
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contact-" class="md-nav__link">
    Contact ðŸŽ£
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain_transformers_pytorch/" class="md-nav__link">
        Pretrain Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_transformers_pytorch/" class="md-nav__link">
        Finetune Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../gpt2_finetune_classification/" class="md-nav__link">
        GPT2 Finetune Classification
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-to-use-pytorchtext-bucketiterator-to-sort-text-data-for-better-batching" class="md-nav__link">
    How to use PyTorchText BucketIterator to sort text data for better batching.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-should-i-know-for-this-notebook" class="md-nav__link">
    What should I know for this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-use-this-notebook" class="md-nav__link">
    How to use this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding" class="md-nav__link">
    Coding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#downloads" class="md-nav__link">
    Downloads
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installs" class="md-nav__link">
    Installs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-pytorch-dataset" class="md-nav__link">
    Using PyTorch Dataset
  </a>
  
    <nav class="md-nav" aria-label="Using PyTorch Dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset-class" class="md-nav__link">
    Dataset Class
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train---validation-datasets" class="md-nav__link">
    Train - Validation Datasets
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-dataloader" class="md-nav__link">
    PyTorch DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorchtext-bucket-iterator-dataloader" class="md-nav__link">
    PyTorchText Bucket Iterator Dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare-dataloaders" class="md-nav__link">
    Compare DataLoaders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train-loop-examples" class="md-nav__link">
    Train Loop Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-pytorchtext-tabulardataset" class="md-nav__link">
    Using PyTorchText TabularDataset
  </a>
  
    <nav class="md-nav" aria-label="Using PyTorchText TabularDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-to-files" class="md-nav__link">
    Data to Files
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tabulardataset" class="md-nav__link">
    TabularDataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorchtext-bucket-iterator-dataloader_1" class="md-nav__link">
    PyTorchText Bucket Iterator Dataloader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compare-dataloaders_1" class="md-nav__link">
    Compare DataLoaders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train-loop-examples_1" class="md-nav__link">
    Train Loop Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-note" class="md-nav__link">
    Final Note
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contact-" class="md-nav__link">
    Contact ðŸŽ£
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/gmihaila/ml_things/tree/master/docs/src/markdown/tutorial_notebooks/pytorchtext_bucketiterator.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>



<h1 id="better-batches-with-pytorchtext-bucketiterator"><img alt="ðŸ‡" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f347.svg" title=":grapes:" /> <strong>Better Batches with PyTorchText BucketIterator</strong><a class="headerlink" href="#better-batches-with-pytorchtext-bucketiterator" title="Permanent link"></a></h1>
<h2 id="how-to-use-pytorchtext-bucketiterator-to-sort-text-data-for-better-batching"><strong>How to use PyTorchText BucketIterator to sort text data for better batching.</strong><a class="headerlink" href="#how-to-use-pytorchtext-bucketiterator-to-sort-text-data-for-better-batching" title="Permanent link"></a></h2>
<p><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> &nbsp;
<a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a>
<a href="https://www.dropbox.com/s/7gyq6qup6y43z9b/pytorchtext_bucketiterator.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a>
<a href="https://gmihaila.medium.com/better-batches-with-pytorchtext-bucketiterator-12804a545e2a"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a>
<a href="https://opensource.org/licenses/Apache-2.0"><img alt="License" src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" /></a></p>
<p><br></p>
<p><strong>Disclaimer:</strong> <em>The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format.</em></p>
<p><br></p>
<p>This notebook is a simple tutorial on how to use the powerful <strong>PytorchText</strong>  <strong>BucketIterator</strong> functionality to group examples (<strong>I use examples and sequences interchangeably</strong>) of similar lengths into batches. This allows us to provide the most optimal batches when training models with text data.</p>
<p>Having batches with similar length examples provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.) where padding will be minimal.</p>
<p>Basically any model that takes as input variable text data sequences will benefit from this tutorial.</p>
<p><strong>I will not train any models in this notebook!</strong> I will release a tutorial where I use this implementation to train a transformer model.</p>
<p>The purpose is to use an example text datasets and batch it using <strong>PyTorchText</strong> with <strong>BucketIterator</strong> and show how it groups text sequences of similar length in batches.</p>
<p>This tutorial has two main parts:</p>
<ul>
<li>
<p><strong>Using PyTorch Dataset with PyTorchText Bucket Iterator</strong>: Here I implemented a standard PyTorch Dataset class that reads in the example text datasets and use PyTorch Bucket Iterator to group similar length examples in same batches. I want to show how easy it is to use this powerful functionality form PyTorchText on a regular PyTorch Dataset workflow which you already have setup.</p>
</li>
<li>
<p><strong>Using PyTorch Text TabularDataset with PyTorchText Bucket Iterator</strong>: Here I use the built-in PyTorchText TabularDataset that reads data straight from local files without the need to create a PyTorch Dataset class. Then I follow same steps as in the previous part to show how nicely text examples are grouped together.</p>
</li>
</ul>
<p><em>This notebooks is a code adaptation and implementation inspired from a few sources:</em> <a href="https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html">torchtext_translation_tutorial</a>, <a href="https://github.com/pytorch/text">pytorch/text - GitHub</a>, <a href="https://torchtext.readthedocs.io/en/latest/index.html#">torchtext documentation</a> and <a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/">A Comprehensive Introduction to Torchtext</a>.</p>
<p><br></p>
<h2 id="what-should-i-know-for-this-notebook"><strong>What should I know for this notebook?</strong><a class="headerlink" href="#what-should-i-know-for-this-notebook" title="Permanent link"></a></h2>
<p>Some basic PyTorch regarding Dataset class and using DataLoaders. Some knowledge of PyTorchText is helpful but not critical in understanding this tutorial. The BucketIterator is similar in applying Dataloader to a PyTorch Dataset.</p>
<p><br></p>
<h2 id="how-to-use-this-notebook"><strong>How to use this notebook?</strong><a class="headerlink" href="#how-to-use-this-notebook" title="Permanent link"></a></h2>
<p>The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks in order to achieve optimal batching.</p>
<p>Comments should provide enough guidance to easily adapt this notebook to your needs.</p>
<p>This code is designed mostly for <strong>classification tasks</strong> in mind, but it can be adapted for any other Natural Language Processing tasks where batching text data is needed.</p>
<p><br></p>
<h2 id="dataset"><strong>Dataset</strong><a class="headerlink" href="#dataset" title="Permanent link"></a></h2>
<p>I will use the well known movies reviews positive - negative labeled <a href="https://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>.</p>
<p>The description provided on the Stanford website:</p>
<p><em>This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.</em></p>
<p><strong>Why this dataset?</strong> I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.</p>
<p><br></p>
<h2 id="coding"><strong>Coding</strong><a class="headerlink" href="#coding" title="Permanent link"></a></h2>
<p>Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevantâ€Š-â€Šshow the output.</p>
<p>I made this format to be easy to follow if you decide to run each code cell in your own python notebook.</p>
<p>When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.</p>
<p><br></p>
<h2 id="downloads"><strong>Downloads</strong><a class="headerlink" href="#downloads" title="Permanent link"></a></h2>
<p>Download the IMDB Movie Reviews sentiment dataset and unzip it locally.</p>
<div class="highlight"><pre><span></span><code># download the dataset
!wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
# unzip it
!tar -zxf /content/aclImdb_v1.tar.gz
</code></pre></div>
<h2 id="installs"><strong>Installs</strong><a class="headerlink" href="#installs" title="Permanent link"></a></h2>
<ul>
<li><strong><a href="https://github.com/gmihaila/ml_things">ml_things</a></strong> library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project.</li>
</ul>
<div class="highlight"><pre><span></span><code># Install helper functions.
!pip install -q git+https://github.com/gmihaila/ml_things.git
</code></pre></div>
<div class="highlight"><pre><span></span><code>|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 5.2MB/s
Building wheel for ml-things (setup.py) ... done
Building wheel for ftfy (setup.py) ... done
</code></pre></div>
<h2 id="imports"><strong>Imports</strong><a class="headerlink" href="#imports" title="Permanent link"></a></h2>
<p>Import all needed libraries for this notebook.</p>
<p>Declare basic parameters used for this notebook:</p>
<ul>
<li>
<p><code>device</code> - Device to use by torch: GPU/CPU. I use CPU as default since I will not perform any costly operations.</p>
</li>
<li>
<p><code>train_batch_size</code> - Batch size used on train data.</p>
</li>
<li>
<p><code>valid_batch_size</code> - Batch size used for validation data. It usually is greater than <code>train_batch_size</code> since the model would only need to make prediction and no gradient calculations is needed.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torchtext</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">ml_things</span> <span class="kn">import</span> <span class="n">fix_text</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="c1"># Will use `cpu` for simplicity.</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>

<span class="c1"># Number of batches for training</span>
<span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Number of batches for validation. Use a larger value than training.</span>
<span class="c1"># It helps speed up the validation process.</span>
<span class="n">valid_batch_size</span> <span class="o">=</span> <span class="mi">20</span>
</code></pre></div>
<h2 id="using-pytorch-dataset"><strong>Using PyTorch Dataset</strong><a class="headerlink" href="#using-pytorch-dataset" title="Permanent link"></a></h2>
<p>This is where I create the PyTorch Dataset objects for training and validation that <strong>can</strong> be used to feed data into a model. This is standard procedure when using PyTorch.</p>
<h3 id="dataset-class"><strong>Dataset Class</strong><a class="headerlink" href="#dataset-class" title="Permanent link"></a></h3>
<p>Implementation of the PyTorch Dataset class.</p>
<p>Most important components in a PyTorch Dataset class are:</p>
<ul>
<li>
<p><code>__len__(self, )</code> where it returns the number of examples in our dataset that we read in <code>__init__(self, )</code>. This will ensure that <code>len()</code> will return the number of examples.</p>
</li>
<li>
<p><code>__getitem__(self, item)</code> where given an index <code>item</code> will return the example corresponding to the <code>item</code> position.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">MovieReviewsTextDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;PyTorch Dataset class for loading data.</span>

<span class="sd">  This is where the data parsing happens.</span>

<span class="sd">  This class is built with reusability in mind.</span>

<span class="sd">  Arguments:</span>

<span class="sd">    path (:obj:`str`):</span>
<span class="sd">        Path to the data partition.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>

    <span class="c1"># Check if path exists.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
      <span class="c1"># Raise error if path is invalid.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid `path` variable! Needs to be a directory&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Since the labels are defined by folders with data we loop</span>
    <span class="c1"># through each label.</span>
    <span class="k">for</span> <span class="n">label</span>  <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">]:</span>
      <span class="n">sentiment_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

      <span class="c1"># Get all files from path.</span>
      <span class="n">files_names</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">sentiment_path</span><span class="p">)</span><span class="c1">#[:10] # Sample for debugging.</span>
      <span class="c1"># Go through each file and read its content.</span>
      <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">files_names</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s1"> Files&#39;</span><span class="p">):</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentiment_path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>

        <span class="c1"># Read content.</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="c1"># Fix any unicode issues.</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">fix_text</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        <span class="c1"># Save content.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        <span class="c1"># Save labels.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

    <span class="c1"># Number of examples.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

    <span class="k">return</span>


  <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;When used `len` return the number of examples.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_examples</span>


  <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given an index return an example from the position.</span>

<span class="sd">    Arguments:</span>

<span class="sd">      item (:obj:`int`):</span>
<span class="sd">          Index position to pick an example to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">      :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed</span>
<span class="sd">      to a model.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">texts</span><span class="p">[</span><span class="n">item</span><span class="p">],</span> <span class="s1">&#39;label&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">item</span><span class="p">]}</span>
</code></pre></div>
<h3 id="train---validation-datasets"><strong>Train - Validation Datasets</strong><a class="headerlink" href="#train---validation-datasets" title="Permanent link"></a></h3>
<p>Create PyTorch Dataset for train and validation partitions.</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dealing with Train...&#39;</span><span class="p">)</span>
<span class="c1"># Create pytorch dataset.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MovieReviewsTextDataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/content/aclImdb/train&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Created `train_dataset` with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s1"> examples!&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dealing with Validation...&#39;</span><span class="p">)</span>
<span class="c1"># Create pytorch dataset.</span>
<span class="n">valid_dataset</span> <span class="o">=</span>  <span class="n">MovieReviewsTextDataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/content/aclImdb/test&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Created `valid_dataset` with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span><span class="si">}</span><span class="s1"> examples!&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Dealing with Train...
pos Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [01:22&lt;00:00, 151.34it/s]
neg Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [01:10&lt;00:00, 178.52it/s]
Created `train_dataset` with 25000 examples!

Dealing with Validation...
pos Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [01:22&lt;00:00, 151.34it/s]
neg Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [01:10&lt;00:00, 178.52it/s]
Created `valid_dataset` with 25000 examples!
</code></pre></div>
<h3 id="pytorch-dataloader"><strong>PyTorch DataLoader</strong><a class="headerlink" href="#pytorch-dataloader" title="Permanent link"></a></h3>
<p>In order to group examples from the PyTorch Dataset into batches we use PyTorch DataLoader. This is standard when using PyTorch.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Move pytorch dataset into dataloader.</span>
<span class="n">torch_train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Created `torch_train_dataloader` with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">torch_train_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s1"> batches!&#39;</span><span class="p">)</span>

<span class="c1"># Move pytorch dataset into dataloader.</span>
<span class="n">torch_valid_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">valid_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Created `torch_valid_dataloader` with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">torch_valid_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s1"> batches!&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Created `torch_train_dataloader` with 2500 batches!
Created `torch_valid_dataloader` with 1250 batches!
</code></pre></div>
<h3 id="pytorchtext-bucket-iterator-dataloader"><strong>PyTorchText Bucket Iterator Dataloader</strong><a class="headerlink" href="#pytorchtext-bucket-iterator-dataloader" title="Permanent link"></a></h3>
<p>Here is where the magic happens! We pass in the <strong>train_dataset</strong> and <strong>valid_dataset</strong> PyTorch Dataset splits into <strong>BucketIterator</strong> to create the actual batches.</p>
<p>It's very nice that PyTorchText can handle splits! No need to write same line of code again for train and validation split.</p>
<p><strong>The <code>sort_key</code> parameter is very important!</strong> It is used to order text sequences in batches. Since we want to batch sequences of text with similar length, we will use a simple function that returns the length of an data example (<code>len(x['text')</code>). This function needs to follow the format of the PyTorch Dataset we created in order to return the length of an example, in my case I return a dictionary with <code>text</code> key for an example.</p>
<p><strong>It is important to keep <code>sort=False</code> and <code>sort_with_batch=True</code> to only sort the examples in each batch and not the examples in the whole dataset!</strong></p>
<p>Find more details in the PyTorchText <strong>BucketIterator</strong> documentation <a href="https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator">here</a> - look at the <strong>BPTTIterator</strong> because it has same parameters except the <strong>bptt_len</strong> argument.</p>
<p><strong>Note:</strong>
<em>If you want just a single DataLoader use <code>torchtext.data.BucketIterator</code> instead of <code>torchtext.data.BucketIterator.splits</code> and make sure to provide just one PyTorch Dataset instead of tuple of PyTorch Datasets and change the parameter <code>batch_sizes</code> and its tuple values to <code>batch_size</code> with single value: <code>dataloader = torchtext.data.BucketIterator(dataset, batch_size=batch_size, )</code></em></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Group similar length text sequences together in batches.</span>
<span class="n">torchtext_train_dataloader</span><span class="p">,</span> <span class="n">torchtext_valid_dataloader</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>

                              <span class="c1"># Datasets for iterator to draw data from</span>
                              <span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">),</span>

                              <span class="c1"># Tuple of train and validation batch sizes.</span>
                              <span class="n">batch_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">valid_batch_size</span><span class="p">),</span>

                              <span class="c1"># Device to load batches on.</span>
                              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>

                              <span class="c1"># Function to use for sorting examples.</span>
                              <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]),</span>


                              <span class="c1"># Repeat the iterator for multiple epochs.</span>
                              <span class="n">repeat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

                              <span class="c1"># Sort all examples in data using `sort_key`.</span>
                              <span class="n">sort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>

                              <span class="c1"># Shuffle data on each epoch run.</span>
                              <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

                              <span class="c1"># Use `sort_key` to sort examples in each batch.</span>
                              <span class="n">sort_within_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="p">)</span>

<span class="c1"># Print number of batches in each split.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Created `torchtext_train_dataloader` with </span><span class="si">%d</span><span class="s1"> batches!&#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">torchtext_train_dataloader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Created `torchtext_valid_dataloader` with </span><span class="si">%d</span><span class="s1"> batches!&#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">torchtext_valid_dataloader</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Created `torchtext_train_dataloader` with 2500 batches!
Created `torchtext_valid_dataloader` with 1250 batches!
</code></pre></div>
<h3 id="compare-dataloaders"><strong>Compare DataLoaders</strong><a class="headerlink" href="#compare-dataloaders" title="Permanent link"></a></h3>
<p>Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches. We can see how nicely examples of similar length are grouped in same batch with PyTorchText.</p>
<p><strong>Note:</strong> <em>When using the PyTorchText BucketIterator, make sure to call <code>create_batches()</code> before looping through each batch! Else you won't get any output form the iterator.</em></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Loop through regular dataloader.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PyTorch DataLoader</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">torch_train_dataloader</span><span class="p">:</span>

  <span class="c1"># Let&#39;s check batch size.</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch size: </span><span class="si">%d</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL</span><span class="se">\t</span><span class="s1">LENGTH</span><span class="se">\t</span><span class="s1">TEXT&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

  <span class="c1"># Print each example.</span>
  <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%d</span><span class="se">\t</span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">text</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="c1"># Only look at first batch. Reuse this code in training models.</span>
  <span class="k">break</span>


<span class="c1"># Create batches - needs to be called before each loop.</span>
<span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">create_batches</span><span class="p">()</span>

<span class="c1"># Loop through BucketIterator.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PyTorchText BuketIterator</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">batches</span><span class="p">:</span>

  <span class="c1"># Let&#39;s check batch size.</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch size: </span><span class="si">%d</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL</span><span class="se">\t</span><span class="s1">LENGTH</span><span class="se">\t</span><span class="s1">TEXT&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

  <span class="c1"># Print each example.</span>
  <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%d</span><span class="se">\t</span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]),</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="c1"># Only look at first batch. Reuse this code in training models.</span>
  <span class="k">break</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>PyTorch DataLoader

Batch size: 10

LABEL LENGTH TEXT
neg   811    Much as we all love Al Pacino, it was painful to see him in this movie. A publicity hack at the grubby ending of what seems to have once been a distinguished and idealistic career Pacino plays his part looking like an unmade bed and assaulting everyone with a totally bogus and inconsistent southern accent.&lt;br /&gt;&lt;br /&gt;The plot spools out this way and that with so many loose ends and improbabilities that the mind reels (and then retreats).&lt;br /&gt;&lt;br /&gt;Kim Basinger is there, not doing much. Her scenes with Pacino are flat and unconvincing. Hard to believe they meant a lot to each other. There&#39;s no energy there.&lt;br /&gt;&lt;br /&gt;Tea Leone, on the other hand, lit up the screen. She was electric and her scenes with Pacino were by far the most interesting in the movie, but not enough to save Al from embarrassment.  
neg   572    God, I am so sick of the crap that comes out of America called &quot;Cartoons&quot;!&lt;br /&gt;&lt;br /&gt;Since anime became popular, the USA animators either produce a cartoon with a 5-year-old-lazy-ass style of drawing (Kappa Mikey) or some cheep Japanese animation rip-off. (Usually messing up classic characters) No wonder anime is beating American cartoons! &lt;br /&gt;&lt;br /&gt;They are not even trying anymore! &lt;br /&gt;&lt;br /&gt;Oh, I just heard of this last night; I live in the UK and when I found out this show first came out in 2005,well, I never knew the UK was so up-to-date with current shows.  
pos   3122   First an explanation on what makes a great movie for me. Excitement about not knowing what is coming next will make me enjoy a movie the first time I watch it (case en point: Twister). There are also other things that go into a great first viewing such as good humor (John Candy in Uncle Buck and The Great Outdoors), good plot with good resolution (Madeline and Matilda), imaginative storytelling (all Star Wars episodes-George Lucas is THE MAN), and good music (again all Star Wars episodes, Wizard of Oz, Sound of Music). What makes me watch a movie at least six times in the theatre and buy a DVD or VHS tape? Characters. With that said, I present Cindy Lou Who and The Grinch. Excellent performance Taylor Momsen and Jim Carrey. The rest of the cast was very good, particularly Jeffery Tambor, Bill Irwin, Molly Shannon, Christine Baranski, and Josh Ryan Evans. But, every single scene with Cindy and The Grinch-together is excellent and very funny and/or heartwarming. Cindy Lou is my favorite character in this movie and the most compelling reason why the movie is better than the cartoon. The Grinch has a strong plot, good conflicts, and a very good theme (I can&#39;t get started because I don&#39;t want to spoil it). Jim Carrey was very funny as The Grinch-particularly when he interacted with Cindy. And the music! Wow! Excellent music by James Horner. I loved his selection of instruments and the compositions. Very good job Jim Carrey-I didn&#39;t know you could sing. Taylor Momsen! Whoa! Your voice is reason enough to see the movie at least once. On your solo - Where Are You Christmas - is your voice really as high as it sounds? Sounds like an F#? That is an obscene range for a 7-year old (obscene meant in the best possible way). Great job. This is the best performance by a child I have ever heard in a movie(Taylor beat out the Von Trapp Children-no small feat!). And now to the actors. Jim Carrey was great, funny, and, surprisingly very sensitive (this really showed through in his scenes with Taylor Momsen). Taylor Momsen&#39;s unspoken expressions(one of the secrets to a good acting performance) are very strong-she really becomes Cindy Lou Who. And when she does dialogue she is even stronger.&lt;br /&gt;&lt;br /&gt;******************************danger:spoiler alert********************* ***********************************************************************&lt;br /&gt;&lt;br /&gt;Examples: expression when she first sees The Grinch. This is a classic quote (&quot;You&#39;re the the the&quot; and then filled in with the Grinch line &quot;da da da THE GRINCH-after which she topples into the sorter and then is rescued by The Grinch). The &quot;Thanks for saving me&quot; quote and subsequent response by The Grinch was also very good.&lt;br /&gt;&lt;br /&gt;My favorite part of the movie is when Cindy invites The Grinch to be Holiday Cheermeister. This scene is two excellent actors at their best interacting and expressing with each other. Little Taylor Momsen completely holds her own with Jim Carrey in this spot. I sincerely hope we see Taylor Momsen in many more films to come. All in all everything was great about this movie (except maybe the feet and noses).  
pos   483    Red Rock West is one of those rare films that keeps you guessing the entire time as to what will happen next. Nicolas Cage is mistaken for a contract killer as he enters a small town trying to find work. Dennis Hopper is the bad guy and no one plays them better. Look for a brief appearance by country singing star Dwight Yoakam. This is a serious drama most of the time but there are some lighter moments. What matters is that you will enjoy this low budget but high quality effort!  
pos   759    This movie is a remake of two movies that were a lot better. The last one, Heaven Can Wait, was great, I suggest you see that one. This one is not so great. The last third of the movie is not so bad and Chris Rock starts to show some of the comic fun that got him to where he is today. However, I don&#39;t know what happened to the first two parts of this movie. It plays like some really bad &quot;B&quot; movie where people sound like they are in some bad TV sit-com. The situations are forced and it is like they are just trying to get the story over so they can start the real movie. It all seems real fake and the editing is just bad. I don&#39;t know how they could release this movie like that. Anyway, the last part isn&#39;t to bad, so wait for the video and see it then.  
pos   2471   VIVAH in my opinion is the best movie of 2006, coming from a director that has proved successful throughout his career. I am not too keen in romantic movies these days, because i see them as &quot;old wine in a new bottle&quot; and so predictable. However, i have watched this movie three times now...and believe me it&#39;s an awesome movie.&lt;br /&gt;&lt;br /&gt;VIVAH goes back to the traditional route, displaying simple characters into a sensible and realistic story of the journey between engagement and marriage. The movie entertains in all manners as it can be reflected to what we do (or would do) when it comes to marriage. In that sense Sooraj R. Barjatya has done his homework well and has depicted a very realistic story into a well-made highly entertaining movie.&lt;br /&gt;&lt;br /&gt;Several sequences in this movie catch your interest immediately: &lt;br /&gt;&lt;br /&gt;* When Shahid Kapoor comes to see the bride (Amrita Rao) - the way he tries to look at her without making it too obvious in front of his and her family. The song &#39;Do Anjaane Ajnabi&#39; goes well with the mood of this scene.&lt;br /&gt;&lt;br /&gt;* The first conversation between Shahid and Amrita, when he comes to see her - i.e. a shy Shahid not knowing exactly what to talk about but pulling of a decent conversation. Also Amrita&#39;s naive nature, limited eye-contact, shy characteristics and answering softly to Shahid&#39;s questions.&lt;br /&gt;&lt;br /&gt;* The emotional breakdown of Amrita and her uncle (Alok Nath) when she feeds him at Shahid&#39;s party in the form of another&#39;s daughter-in-law rather than her uncle&#39;s beloved niece.&lt;br /&gt;&lt;br /&gt;Clearly the movie belongs to Amrita Rao all the way. The actress portrays the role of Poonam with such conviction that you cannot imagine anybody else replacing her. She looks beautiful throughout the whole movie, and portrays an innocent and shy traditional girl perfectly.&lt;br /&gt;&lt;br /&gt;Shahid Kapoor performs brilliantly too. He delivers a promising performance and shows that he is no less than Salman Khan when it comes to acting in a Sooraj R. Barjatya film. In fact Shahid and Amrita make a cute on-screen couple, without a shadow of doubt. Other characters - Alok Nath (Excellent), Anupam Kher (Brilliant), Mohan Joshi (Very good).&lt;br /&gt;&lt;br /&gt;On the whole, VIVAH delivers what it promised, a well made and realistic story of two families. The movie has top-notch performances, excellent story and great music to suit the film, as well as being directed by the fabulous Sooraj R. Barjatya. It&#39;s a must see!  
neg   626    Watching this Movie? l thought to myself, what a lot of garbage. These girls must have rocks for brains for even agreeing to be part of it. Waste of time watching it, faint heavens l only hired it. The acting was below standard and story was unbearable. Anyone contemplating watching this film, please save your money. The film has no credit at all. l am a real film buff and this is worse than &quot;Attack of the Green Tomatoes&quot;.&lt;br /&gt;&lt;br /&gt;l only hope that this piece of trash didn&#39;t cost too much to make. Money would have been better spent on the homeless people of the world. l only hope there isn&#39;t a sequel in the pipeline.  
pos   2599   A SPECIAL DAY (Ettore Scola - Italy/Canada 1977).&lt;br /&gt;&lt;br /&gt;Every once in a while, you come across a film that really touches a nerve. This one offers a very simple premise, almost flawlessly executed in every way and incredibly moving at the same time. It&#39;s surprising Ettore Scola&#39;s &quot;Una giornate particulare&quot; is relatively unheralded, even hated by some critics. Time Out calls it &#39;rubbish&#39; and Leonard Maltin, somewhat milder, &#39;pleasant but trifling.&#39; I disagree, not only because this film is deeply moving, but within its simple story it shows us more insights about daily life in fascist Italy than most films I&#39;ve seen. The cinematography is distinctly unflashy, even a bit bland, and the storyline straightforward, which might explain the film&#39;s relative unpopularity. Considering late &#39;70s audiences weren&#39;t exactly spoiled with great Italian films, it&#39;s even stranger this one didn&#39;t really catch on with the critics.&lt;br /&gt;&lt;br /&gt;The film begins with a ten-minute collage of archive footage from Hitler&#39;s visit to Italy on may 8th 1938. Set against this background, we first meet Antonietta (Loren), a lonely, love-ridden housewife with six children in a roman apartment building. One day, when her Beo escapes, she meets her neighbour Gabriele (Mastroianni), who seems to be only one in the building not attending the ceremonies. He is well-mannered, cultured and soon she is attracted to him. During the whole film, we hear the fascist rally from the radio of the concierge hollering through the courtyard. Scola playfully uses the camera to make us part of the proceedings. After the opening scene, the camera swanks across the courtyard of the modernist (hypermodern at the time) apartment block, seemingly searching for our main characters, whom we haven&#39;t met yet. &lt;br /&gt;&lt;br /&gt;Marcello Mastrionani and Sophia Loren are unforgettable in the two leading roles, all the more astonishing since they are cast completely against type. Canadian born John Vernon plays Loren&#39;s husband, but he is only on screen in the first and last scene. I figure his voice must have been dubbed, since he&#39;s not of Italian descent and never lived there, to my knowledge, so I cannot imagine he speaks Italian. If his voice has been dubbed, I didn&#39;t notice at all. On the contrary, he&#39;s completely believable as an Italian, even more than the rest of the cast. The story is simple but extremely effective, the performances are outstanding, the ending is just perfect and the framing doesn&#39;t come off as overly pretentious but works completely. Don&#39;t miss out on this one.&lt;br /&gt;&lt;br /&gt;Camera Obscura --- 9/10  
neg   1482   There are some extremely talented black directors Spike Lee,Carl Franklin,Billy Dukes,Denzel and a host of others who bring well deserved credit to the film industry . Then there are the Wayans Brothers who at one time(15,years ago) had an extremely funny television show&#39;In Living Colour&#39; that launched the career of Jim Carrey amongst others . Now we have stupidity substituting for humour and gross out gags(toilet humour) as the standard operating procedure . People are not as stupid as those portrayed in &#39;Little Man&#39; they couldn&#39;t possibly be . A baby with a full set of teeth and a tattoo is accepted as being only months old ? Baby comes with a five o&#39;clock shadow that he shaves off . It is intimated that the baby has sex with his foster mother behind her husbands,Darryl&#39;s, back .Oh, yea that is just hilarious . As a master criminal &#39;Little Man&#39; is the stupidest on planet earth . He stashes a stolen rock that is just huge in a woman&#39;s purse and then has to pursue her . Co-star Chazz Palminteri,why Chazz, offers the best line: &quot;I&#39;m surrounded by morons.&quot; Based, without credit, on a Chuck Jones cartoon, Baby Buggy Bunny . This is far too stupid to be even remotely funny . A clue as to how bad this film is Damon Wayans appeared on Jay Leno the other night,prior to the BAT awards and he did not,even mention this dreadful movie . When will Hollywood stop green lighting trash from the Wayans Brothers . When they get over their white mans guilt in all likelihood .  
neg   4380   There is a bit of a spoiler below, which could ruin the surprise of the ONE unexpected and truly funny scene in this film. There is also information about the first film in this series.&lt;br /&gt;&lt;br /&gt;I caught this film on DVD, which someone gave as a gift to my roommate. It came as a set together with the first film in the &quot;Blind Dead&quot; series.&lt;br /&gt;&lt;br /&gt;This movie was certainly much worse than the first, &quot;La Noche del Terror Ciego&quot;. In addition, many of the features of the first movie were changed significantly. To boot, the movie was dubbed in English (the first was subtitled), which I tend to find distracting.&lt;br /&gt;&lt;br /&gt;The concept behind the series is that in the distant past a local branch of the Knights Templar was involved in heinous and secret rituals. Upon discovery of these crimes, the local peasantry put the Templars to death in such a manner that their eyes can no longer be used, thus preventing them from returning from Hell to exact their revenge. We then jump to modern times where because of some event, the Templars arise from the dead to exact their revenge upon the villagers whose ancestors messed them up in the first place. Of course, since the undead knights have no eyes, they can only find their victims when they make some sort of noise.&lt;br /&gt;&lt;br /&gt;The Templars were a secretive order, from about the 12th century, coming out of the Crusades. They were only around for about 150 years, before they were suppressed in the early 1300s by the Pope and others. Because they were secretive, there were always rumors about their ceremonies, particularly for initiation. Also, because of the way the society was organized, you didn&#39;t necessarily have church officials overseeing things, which meant they didn&#39;t have an inside man when things heated up. And, because of the nature of their trials, they were tortured into confessions. The order was strongest in France, but did exist in Portugal and Spain, where the movies take place.&lt;br /&gt;&lt;br /&gt;Where the first movie had a virgin sacrifice and knights drinking the blood directly from the body of the virgin (breast shots here, of course, this is a horror film after all), and then, once the knights come back to life, they attack their victims by eating them alive and sucking their blood; in this sequel, this all disappears. You still have the same scene (redone, not the same footage) of them sacrificing the virgin, but they drain the blood into a bowl and drink it from that. Thus, when they come back, they just hack people up with their swords or claw people to death, which I have to say is a much less effective means of disturbing your audience. There&#39;s also a time problem: in the first film the dating is much closer to the Templars, where here they are now saying it is the 500 anniversary of the peasants burning these guys at the stake, which would date it around 1473. And the way that the Templars lose their eyes is much less interesting as well. In the first, they have them pecked out by crows. Now they are simply burned out, and in quite a ridiculous manner.&lt;br /&gt;&lt;br /&gt;Oh yeah, and maybe it was just me, but there seemed to be a lot of people from the first movie reappearing in this film (despite having died). Not really a problem, since the movie is completely different and not a sequel in the sense of a continuation, but odd none-the-less.&lt;br /&gt;&lt;br /&gt;The highlight of this movie is the rich fellow who uses a child to distract the undead while he makes a break for the jeep. The child&#39;s father had already been suckered by this rich man into making an attempt to get the jeep, so he walks out and tells her to find her father. It comes somewhat out of the blue, and is easily the funniest scene in the film. Of course, why the child doesn&#39;t die at this point is beyond me, and disappointed for horror fans.&lt;br /&gt;&lt;br /&gt;I couldn&#39;t possibly recommend this film to anyone. It isn&#39;t so bad that it becomes funny, so it just ends up being a mediocre horror film. The bulk of the film has several people holed up in a church, each making various attempts to go it alone in order to escape the blind dead who have them surrounded. When the film ends, you are not surprised at the outcome at all; in fact, quite disappointed. If you are into the novelty of seeing a Spanish horror film, see the first movie, which at least has some innovative ideas and not so expected outcomes.


PyTorchText BuketIterator

Batch size: 10

LABEL LENGTH TEXT
neg   1118   Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet&#39;s documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.&lt;br /&gt;&lt;br /&gt;The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.&lt;br /&gt;&lt;br /&gt;Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals.  
neg   1120   Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),&quot;Connie &amp; Carla&quot;,&#39;04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),&quot;Taking Lives&quot;,&#39;04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),&quot;Kinsey&quot;,&#39;04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),&quot;CSI-Vegas TV Series&quot;, plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain.  
pos   1120   I must say that, looking at Hamlet from the perspective of a student, Brannagh&#39;s version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.&lt;br /&gt;&lt;br /&gt;One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.&lt;br /&gt;&lt;br /&gt;Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it.  
pos   1120   As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While &quot;Angels in the Outfield&quot; fails miserably in the first category, it succeeds beautifully in the latter two. &quot;Angels&quot; weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father&#39;s promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise&#39;s downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup!  
neg   1121   I have a piece of advice for the people who made this movie too, if you&#39;re gonna make a movie like this be sure you got the f/x to back it up. Also don&#39;t get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don&#39;t blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let&#39;s talk about the acting for just a second, the Carradine guy who&#39;s career peaked in 1984 when he did &quot;Revenge of the Nerds&quot; (which was actually a great comedy). He&#39;s not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after &quot;Return of the Jedi&quot; and go quietly into the night. He made his mark as a &quot;Nerd&quot; and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I&#39;ll close on that note. Thank you for your time.  
neg   1121   By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That&#39;s the best thing that could be said about &quot;Adam Had Four Sons,&quot; a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter&#39;s four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it&#39;s a relief when Susan Hayward as the villainess enters midway through â€” she&#39;s about the only watchable person in the movie even though she&#39;s clearly channeling Bette Davis and Vivien Leigh; it&#39;s also the first in her long succession of alcoholic roles â€” but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted &quot;The Valley of Decision&quot; four years later.  
neg   1123   I have never read the book&quot;A wrinkle in time&quot;. To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn&#39;t be reviewing this movie i&#39;ll start off with that. Next i&#39;ll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than &quot;Merlin&quot;.&lt;br /&gt;&lt;br /&gt;How do I describe a TV movie? I have never written a review for one before. Well, i&#39;ll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. &lt;br /&gt;&lt;br /&gt;The film has cheesy special effects, a mildly interesting plot, scenes that make you go &quot;WTF&quot;. The movie is incredibly bad and it makes you go&quot;WTF&quot;. What did I expect? It&#39;s a TV movie. They usually aren&#39;t good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you&#39;ll most likely forget about it the second it&#39;s over. Well, maybe not the second it&#39;s over. But within a few minutes.&lt;br /&gt;&lt;br /&gt;A wrinkle in time:*/****  
neg   1123   After watching &quot;The Bodyguard&quot; last night, I felt compelled to write a review of it.&lt;br /&gt;&lt;br /&gt;This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.&lt;br /&gt;&lt;br /&gt;The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee&#39;s involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.&lt;br /&gt;&lt;br /&gt;The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I&#39;m sure this would have been a classic Kung Foo movie. As it is, it&#39;s more like another cheesy 70&#39;s action movie.&lt;br /&gt;&lt;br /&gt;Note: The opening sequence has a quote familiar to &quot;Pulp Fiction&quot; fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.&lt;br /&gt;&lt;br /&gt;Rating: 4 out of 10  
neg   1123   There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.&lt;br /&gt;&lt;br /&gt;Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by &quot;mayor&quot; Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren&#39;t bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth).  
neg   1124   The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).&lt;br /&gt;&lt;br /&gt;The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.&lt;br /&gt;&lt;br /&gt;This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :&lt;br /&gt;&lt;br /&gt;After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?&lt;br /&gt;&lt;br /&gt;Ajay Devgun is total miscast for portraying a rockstar.&lt;br /&gt;&lt;br /&gt;Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. &lt;br /&gt;&lt;br /&gt;Watch it maybe only for the last half hour.
</code></pre></div>
<h3 id="train-loop-examples"><strong>Train Loop Examples</strong><a class="headerlink" href="#train-loop-examples" title="Permanent link"></a></h3>
<p>Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset!</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example of number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Example of loop through each epoch</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

  <span class="c1"># Create batches - needs to be called before each loop.</span>
  <span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">create_batches</span><span class="p">()</span>

  <span class="c1"># Loop through BucketIterator.</span>
  <span class="k">for</span> <span class="n">sample_id</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">batches</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch examples lengths: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">%</span> <span class="nb">str</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]))</span>

    <span class="c1"># Let&#39;s break early, you get the idea.</span>
    <span class="k">if</span> <span class="n">sample_id</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
      <span class="k">break</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Batch examples lengths: [791, 792, 792, 793, 797, 797, 799, 799, 801, 801]
Batch examples lengths: [4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313, 5450]
Batch examples lengths: [695, 696, 696, 696, 697, 699, 699, 700, 700, 701]
Batch examples lengths: [960, 961, 963, 963, 963, 966, 966, 967, 968, 969]
Batch examples lengths: [1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221, 1226, 1229]
Batch examples lengths: [2639, 2651, 2651, 2672, 2692, 2704, 2707, 2712, 2720, 2724]
Batch examples lengths: [1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878, 1889, 1895]
Batch examples lengths: [3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278, 3294, 3334]
Batch examples lengths: [3001, 3031, 3039, 3047, 3056, 3077, 3084, 3103, 3104, 3107]
Batch examples lengths: [1053, 1053, 1056, 1057, 1060, 1067, 1073, 1077, 1078, 1080]
Batch examples lengths: [751, 751, 756, 758, 759, 760, 761, 762, 763, 764]
</code></pre></div>
<h2 id="using-pytorchtext-tabulardataset"><strong>Using PyTorchText TabularDataset</strong><a class="headerlink" href="#using-pytorchtext-tabulardataset" title="Permanent link"></a></h2>
<p>Now I will use the TabularDataset functionality which creates the PyTorchDataset object right from our local files.</p>
<p>We don't need to create a custom PyTorch Dataset class to load our dataset as long as we have tabular files of our data.</p>
<h3 id="data-to-files"><strong>Data to Files</strong><a class="headerlink" href="#data-to-files" title="Permanent link"></a></h3>
<p>Since our dataset is scattered into multiple files, I created a function <code>files_to_tsv</code> which puts our dataset into a <code>.tsv</code> file (Tab-Separated Values).</p>
<p>Since I'll use the <strong>TabularDataset</strong> from <code>pytorch.data</code> I need to pass tabular format files.</p>
<p>For text data I find the Tab Separated Values format easier to deal with.</p>
<p>I will call the <code>files_to_tsv</code> function for each of the two partitions <strong>train</strong> and <strong>test</strong>.</p>
<p>The function will return the name of the <code>.tsv</code> file saved so we can use it later in PyTorchText.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">files_to_tsv</span><span class="p">(</span><span class="n">partition_path</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s1">&#39;./&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Parse each file in partition and keep track of sentiments.</span>
<span class="sd">  Create a list of pairs [tag, text]</span>

<span class="sd">  Arguments:</span>

<span class="sd">    partition_path (:obj:`str`):</span>
<span class="sd">      Partition used: train or test.</span>

<span class="sd">    save_path (:obj:`str`):</span>
<span class="sd">      Path where to save the final .tsv file.</span>

<span class="sd">  Returns:</span>

<span class="sd">    :obj:`str`: Filename of created .tsv file.</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># List of all examples in format [tag, text].</span>
  <span class="n">examples</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Print partition.</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">partition_path</span><span class="p">)</span>

  <span class="c1"># Loop through each sentiment.</span>
  <span class="k">for</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">,</span> <span class="s1">&#39;neg&#39;</span><span class="p">]:</span>

    <span class="c1"># Find path for sentiment.</span>
    <span class="n">sentiment_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">partition_path</span><span class="p">,</span> <span class="n">sentiment</span><span class="p">)</span>

    <span class="c1"># Get all files from path sentiment.</span>
    <span class="n">files_names</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">sentiment_path</span><span class="p">)</span>

    <span class="c1"># For each file in path sentiment.</span>
    <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">files_names</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="s1"> Files&#39;</span><span class="p">):</span>

      <span class="c1"># Get file content.</span>
      <span class="n">file_content</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentiment_path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

      <span class="c1"># Fix any format errors.</span>
      <span class="n">file_content</span> <span class="o">=</span> <span class="n">fix_text</span><span class="p">(</span><span class="n">file_content</span><span class="p">)</span>

      <span class="c1"># Append sentiment and file content.</span>
      <span class="n">examples</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">sentiment</span><span class="p">,</span> <span class="n">file_content</span><span class="p">])</span>

  <span class="c1"># Create a TSV file with same format `sentiment  text`.</span>
  <span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%s</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">example</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">]</span>

  <span class="c1"># Create file name.</span>
  <span class="n">tsv_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">partition_path</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_pos_neg_</span><span class="si">%d</span><span class="s1">.tsv&#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

  <span class="c1"># Write to TSV file.</span>
  <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">tsv_filename</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">examples</span><span class="p">))</span>

  <span class="c1"># Return TSV file name.</span>
  <span class="k">return</span> <span class="n">tsv_filename</span>


<span class="c1"># Path where to save tsv file.</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;/content&#39;</span>

<span class="c1"># Convert train files to tsv file.</span>
<span class="n">train_filename</span> <span class="o">=</span> <span class="n">files_to_tsv</span><span class="p">(</span><span class="n">partition_path</span><span class="o">=</span><span class="s1">&#39;/content/aclImdb/train&#39;</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="n">data_path</span><span class="p">)</span>

<span class="c1"># Convert test files to tsv file.</span>
<span class="n">test_filename</span> <span class="o">=</span> <span class="n">files_to_tsv</span><span class="p">(</span><span class="n">partition_path</span><span class="o">=</span><span class="s1">&#39;/content/aclImdb/test&#39;</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="n">data_path</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>/content/aclImdb/train
pos Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:34&lt;00:00, 367.26it/s]
neg Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:21&lt;00:00, 573.00it/s]

/content/aclImdb/test
pos Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:11&lt;00:00, 1075.80it/s]
neg Files: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:12&lt;00:00, 1037.94it/s]
</code></pre></div>
<h3 id="tabulardataset"><strong>TabularDataset</strong><a class="headerlink" href="#tabulardataset" title="Permanent link"></a></h3>
<p>Here I setup the data fields for PyTorchText. We have to tell the library how to handle each column of the <code>.tsv</code> file. For this we need to create <code>data.Field</code> objects for each column.</p>
<p><code>text_tokenizer</code>:
For this example I don't use an actual tokenizer for the <code>text</code> column but I need to create one because it requires as input. I created a dummy tokenizer that returns same value. Depending on the project, here is where you will have your own tokenizer. It needs to take as input text and output a list.</p>
<p><code>label_tokenizer</code>
The label tokenizer is also a dummy tokenizer. This is where you will have a encoder to transform labels to ids.</p>
<p>Since we have two <code>.tsv</code> files it's great that we can use the <code>.split</code> function from <strong>TabularDataset</strong> to handle two files at the same time one for train and the other one for test.</p>
<p>Find more details about <strong>torchtext.data</strong> functionality <a href="https://torchtext.readthedocs.io/en/latest/data.html#dataset-batch-and-example">here</a>.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Text tokenizer function - dummy tokenizer to return same text.</span>
<span class="c1"># Here you will use your own tokenizer.</span>
<span class="n">text_tokenizer</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span>

<span class="c1"># Label tokenizer - dummy label encoder that returns same label.</span>
<span class="c1"># Here you will add your own label encoder.</span>
<span class="n">label_tokenizer</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>

<span class="c1"># Data field for text column - invoke tokenizer.</span>
<span class="n">TEXT</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="n">text_tokenizer</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Data field for labels - invoke tokenize label encoder.</span>
<span class="n">LABEL</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="n">label_tokenizer</span><span class="p">,</span> <span class="n">use_vocab</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create data fields as tuples of description variable and data field.</span>
<span class="n">datafields</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">),</span>
              <span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="n">TEXT</span><span class="p">)]</span>

<span class="c1"># Since we have have tab separated data we use TabularDataset</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TabularDataset</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>

                                                <span class="c1"># Path to train and validation.</span>
                                                <span class="n">path</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>

                                                <span class="c1"># Train data filename.</span>
                                                <span class="n">train</span><span class="o">=</span><span class="n">train_filename</span><span class="p">,</span>

                                                <span class="c1"># Validation file name.</span>
                                                <span class="n">validation</span><span class="o">=</span><span class="n">test_filename</span><span class="p">,</span>

                                                <span class="c1"># Format of local files.</span>
                                                <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;tsv&#39;</span><span class="p">,</span>

                                                <span class="c1"># Check if we have header.</span>
                                                <span class="n">skip_header</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>

                                                <span class="c1"># How to handle fields.</span>
                                                <span class="n">fields</span><span class="o">=</span><span class="n">datafields</span><span class="p">)</span>
</code></pre></div>
<h3 id="pytorchtext-bucket-iterator-dataloader_1"><strong>PyTorchText Bucket Iterator Dataloader</strong><a class="headerlink" href="#pytorchtext-bucket-iterator-dataloader_1" title="Permanent link"></a></h3>
<p>I'm using same setup as in the <strong>PyTorchText Bucket Iterator Dataloader</strong> code cell section. The only difference is in the <code>sort_key</code> since there is different way to access example attributes (we had dictionary format before).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Group similar length text sequences together in batches.</span>
<span class="n">torchtext_train_dataloader</span><span class="p">,</span> <span class="n">torchtext_valid_dataloader</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>

                              <span class="c1"># Datasets for iterator to draw data from</span>
                              <span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">),</span>

                              <span class="c1"># Tuple of train and validation batch sizes.</span>
                              <span class="n">batch_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">valid_batch_size</span><span class="p">),</span>

                              <span class="c1"># Device to load batches on.</span>
                              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>

                              <span class="c1"># Function to use for sorting examples.</span>
                              <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">text</span><span class="p">),</span>


                              <span class="c1"># Repeat the iterator for multiple epochs.</span>
                              <span class="n">repeat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

                              <span class="c1"># Sort all examples in data using `sort_key`.</span>
                              <span class="n">sort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>

                              <span class="c1"># Shuffle data on each epoch run.</span>
                              <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

                              <span class="c1"># Use `sort_key` to sort examples in each batch.</span>
                              <span class="n">sort_within_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="p">)</span>

<span class="c1"># Print number of batches in each split.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Created `torchtext_train_dataloader` with </span><span class="si">%d</span><span class="s1"> batches!&#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">torchtext_train_dataloader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Created `torchtext_valid_dataloader` with </span><span class="si">%d</span><span class="s1"> batches!&#39;</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">torchtext_valid_dataloader</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Created `torchtext_train_dataloader` with 2500 batches!
Created `torchtext_valid_dataloader` with 1250 batches!
</code></pre></div>
<h3 id="compare-dataloaders_1"><strong>Compare DataLoaders</strong><a class="headerlink" href="#compare-dataloaders_1" title="Permanent link"></a></h3>
<p>Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches created with TabularDataset. We can see how nicely examples of similar length are grouped in same batch with PyTorchText.</p>
<p><strong>Note:</strong> <em>When using the PyTorchText BucketIterator, make sure to call <code>create_batches()</code> before looping through each batch! Else you won't get any output form the iterator.</em></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Loop through regular dataloader.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PyTorch DataLoader</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">torch_train_dataloader</span><span class="p">:</span>

  <span class="c1"># Let&#39;s check batch size.</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch size: </span><span class="si">%d</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL</span><span class="se">\t</span><span class="s1">LENGTH</span><span class="se">\t</span><span class="s1">TEXT&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

  <span class="c1"># Print each example.</span>
  <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%d</span><span class="se">\t</span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">text</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="c1"># Only look at first batch. Reuse this code in training models.</span>
  <span class="k">break</span>


<span class="c1"># Create batches - needs to be called before each loop.</span>
<span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">create_batches</span><span class="p">()</span>

<span class="c1"># Loop through BucketIterator.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PyTorchText BuketIterator</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">batches</span><span class="p">:</span>

  <span class="c1"># Let&#39;s check batch size.</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch size: </span><span class="si">%d</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LABEL</span><span class="se">\t</span><span class="s1">LENGTH</span><span class="se">\t</span><span class="s1">TEXT&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

  <span class="c1"># Print each example.</span>
  <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="se">\t</span><span class="si">%d</span><span class="se">\t</span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">example</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="o">.</span><span class="n">text</span><span class="p">),</span> <span class="n">example</span><span class="o">.</span><span class="n">text</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="c1"># Only look at first batch. Reuse this code in training models.</span>
  <span class="k">break</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>PyTorch DataLoader

Batch size: 10

LABEL LENGTH TEXT
pos   1742   As a child I preferred the first Care Bear movie since this one seemed so dark. I always sat down and watched the first one. As I got older I learned to prefer this one. What I do think is that this film is too dark for infants, but as you get older you learn to treasure it since you understand it more, it doesn&#39;t seem as dark as it was back when you were a child.&lt;br /&gt;&lt;br /&gt;This movie, in my opinion, is better than the first one, everything is so much deeper. It may contradict the first movie but you must ignore the first movie to watch this one. The cubs are just too adorable, I rewind that &#39;Flying My Colors&#39; scene. I tend to annoy everyone by singing it.&lt;br /&gt;&lt;br /&gt;The sound track is great! A big hand to Carol and Dean Parks. I love every song in this movie, I have downloaded them all and is all I am listening to, I&#39;m listening to &#39;Our beginning&#39; also known as &#39;Recalling&#39; at the moment. I have always preferred this sound track to the first one, although I just totally love Carol Kings song in the first movie &#39;Care-A-Lot&#39;.&lt;br /&gt;&lt;br /&gt;I think the animation is great, the animation in both movies are fantastic. I was surprised when I sat down and watched it about 10 years later and saw that the animation for the time was excellent. It was really surprising.&lt;br /&gt;&lt;br /&gt;There is not a lot of back up from other people to say that this movie is great, but it is. I do not think it is weird/strange. I think it is a wonderful movie.&lt;br /&gt;&lt;br /&gt;Basically, this movie is about how the Care Bears came about and to defeat the Demon, Dark Heart. The end is surprising and again, beats any &#39;Pokemon Movie&#39; with the Care Bears Moral issues. It leaves an effect on you. Again this movie can teach everyone at all ages about morality.  
pos   1475   Worry not, Disney fans--this special edition DVD of the beloved Cinderella won&#39;t turn into a pumpkin at the strike of midnight. One of the most enduring animated films of all time, the Disney-fide adaptation of the gory Brothers Grimm fairy tale became a classic in its own right, thanks to some memorable tunes (including &quot;A Dream Is a Wish Your Heart Makes,&quot; &quot;Bibbidi-Bobbidi-Boo,&quot; and the title song) and some endearingly cute comic relief. The famous slipper (click for larger image) We all know the story--the wicked stepmother and stepsisters simply won&#39;t have it, this uppity Cinderella thinking she&#39;s going to a ball designed to find the handsome prince an appropriate sweetheart, but perseverance, animal buddies, and a well-timed entrance by a fairy godmother make sure things turn out all right. There are a few striking sequences of pure animation--for example, Cinderella is reflected in bubbles drifting through the air--and the design is rich and evocative throughout. It&#39;s a simple story padded here agreeably with comic business, particularly Cinderella&#39;s rodent pals (dressed up conspicuously like the dwarf sidekicks of another famous Disney heroine) and their misadventures with a wretched cat named Lucifer. There&#39;s also much harrumphing and exposition spouting by the King and the Grand Duke. It&#39;s a much simpler and more graceful work than the more frenetically paced animated films of today, which makes it simultaneously quaint and highly gratifying.  
pos   1279   Seldom do I ever encounter a film so completely fulfilling that I must speak about it immediately. This movie is definitely some of the finest entertainment available and it is highly authentic. I happened to see the dubbed version but I&#39;m on my way right now to grab the DVD remaster with original Chinese dialogue. Still, the dubbing didn&#39;t get in the way and sometimes provided some seriously funny humour: &quot;Poison Clan rocks the world!!!&quot;&lt;br /&gt;&lt;br /&gt;The story-telling stays true to Chinese methods of intrigue, suspense, and inter-personal relationships. You can expect twists and turns as the identities of the 5 venoms are revealed and an expert pace.&lt;br /&gt;&lt;br /&gt;The martial arts fight choreography is in a class of its own and must be seen to be believed. It&#39;s like watching real animals fight each other, but construed from their own arcane martial arts forms. Such level of skill amongst the cast is unsurpassed in modern day cinema.&lt;br /&gt;&lt;br /&gt;The combination provides for a serious dose of old Chinese culture and I recommend it solely on the basis of the film&#39;s genuine intent to tell a martial arts story and the mastery of its execution. ...Of course, if you just want to see people pummel each other, along with crude forms of ancient Chinese torture, be my guest!  
pos   1071   I&#39;m sure that most people already know the story-the miserly Ebenezer Scrooge gets a visit from three spirits (the Ghosts of Christmas Past, Present and Yet to Come) who highlight parts of his life in the hopes of saving his soul and changing his ways. Dickens&#39; classic story in one form or another has stood the test of time to become a beloved holiday favorite.&lt;br /&gt;&lt;br /&gt;While I grew up watching the 1951 version starring Alastair Sims, and I believe that he is the definitive Scrooge, I have been impressed with this version, which was released when I was in high school. George C. Scott plays a convincing and mean Ebenezer Scrooge, and the actors playing the ghosts are rather frightening and menacing. David Warner is a good Bob Cratchit as well.&lt;br /&gt;&lt;br /&gt;This version is beautifully filmed, and uses more modern filming styles (for the 1980&#39;s) which make it more palatable for my children than the 1951 black and white version.&lt;br /&gt;&lt;br /&gt;This is a worthy adaptation of the story and is one that I watch almost every year at some point in the Christmas season.  
neg   876    What was an exciting and fairly original series by Fox has degraded down to meandering tripe. During the first season, Dark Angel was on my weekly &quot;must see&quot; list, and not just because of Jessica Alba.&lt;br /&gt;&lt;br /&gt;Unfortunately, the powers-that-be over at Fox decided that they needed to &quot;fine-tune&quot; the plotline. Within 3 episodes of the season opener, they had totally lost me as a viewer (not even to see Jessica Alba!). I found the new characters that were added in the second season to be too ridiculous and amateurish. The new plotlines were stretching the continuity and credibility of the show too thin. On one of the second season episodes, they even had Max sleeping and dreaming - where the first season stated she biologically couldn&#39;t sleep.&lt;br /&gt;&lt;br /&gt;The moral of the story (the one that Hollywood never gets): If it works, don&#39;t screw with it!&lt;br /&gt;&lt;br /&gt;azjazz  
pos   1981   Greta Garbo&#39;s American film debut is an analogy of how our lives can be swept off course by fate and our actions, as in a torrent, causing us to lose a part of ourselves along the way.&lt;br /&gt;&lt;br /&gt;Greta plays Leonora, a poor peasant girl in love with Ricardo Cortez&#39;s character Don Rafael, a landowner. Ricardo is in love with her too, but is too easily influenced by his domineering mother. Leonora ends up homeless and travels to Paris, where she becomes a famous opera singer and develops the reputation for being a loose woman. In reality, part of her attitude is bitterness over Rafael&#39;s abandonment.&lt;br /&gt;&lt;br /&gt;She returns to her home to visit her family and eventually confronts Rafael. Surprisingly, no one knows that she&#39;s the famous La Brunna, and Garbo acts up her role as the diva she truly was and re prised with such cool haughtiness in her later portrayals.&lt;br /&gt;&lt;br /&gt;Ricardo Cortez reminds one a lot of Valentino in looks in this part, and he was groomed to be a Valentino clone by MGM, though he never thought he could be in reality and he was right. He is believable in an unsympathetic part as a weak willed Mama&#39;s boy, and allows himself to age realistically but comically at the end of the movie. He fails to win Leonora when she returns home, and later when he follows her, his courage is undermined.&lt;br /&gt;&lt;br /&gt;This movie is beautifully shot, with brilliant storm sequences and the sets depicting Spain at the time are authentic looking. There are also some fine secondary performances by old timers Lucien Littlefield, Tully Marshall, and Mack Swain.&lt;br /&gt;&lt;br /&gt;Although this is a story of lost love and missed chances, I don&#39;t think Leonora and Rafael would have been happy together, as he needed a more traditional wife and she was very much a career woman, and I don&#39;t think would have been happy in a small village. The ending is true to life and pulls no punches.&lt;br /&gt;&lt;br /&gt;See this one as Garbo&#39;s American film debut and a precursor of things to come  
pos   1007   *What I Like About SPOILERS* Teenager Holly Tyler (Amanda Bynes) goes to live with older sister Valerie (Jennie Garth) to avoid moving to Japan with her father; but she doesn&#39;t know the half of the wacky things that will happen to her from now on, and not only to her, but to her sister, her friends Gary (Wesley Jonathan) and Tina (Alison Munn), boyfriend Henry (Michael McMillian), crush Vince (Nick Zano), Valerie&#39;s boyfriend Jeff (Simon Rex), first boss (then firefighter then husband) Vic (Dan Cortese), annoying colleague Lauren (Leslie Grossman) and second boss Peter (?) If you don&#39;t have a funny bone in your body, please skip this; if you like only veeeery sophisticated comedy this isn&#39;t for you; if you like a funny, sometimes touching show with two hot chicks who can act in the lead (and none other than the fabulous &#39;Mary Cherry&#39; from Popular - Leslie Grossman - in the main cast), then what the hell are you waiting for? You&#39;re welcome to Casa De Tyler! What I Like About You (2002-2006): 8.  
pos   318    This movie is wonderful. The writing, directing, acting all are fantastic. Very witty and clever script. Quality performances by actors, Ally Sheedy is strong and dynamic and delightfully quirky. Really original and heart-warmingly unpredicatable. The scenes are alive with fresh energy and really talented production.  
pos   1846   In Le Million, Rene Clair, one of the cinema&#39;s great directors and great pioneers, created a gem of light comedy which for all its lightness is a groundbreaking and technically brilliant film which clearly influenced subsequent film-makers such as the Marx Brothers, Lubitsch, and Mamoulian. The plot, a witty story of a poor artist who wins a huge lottery jackpot but has to search frantically all over town for the missing ticket, is basically just a device to support a series of wonderfully witty comic scenes enacted in a dream world of the director&#39;s imagination.&lt;br /&gt;&lt;br /&gt;One of the most impressive things about this film is that, though it is set in the middle of Paris and includes nothing actually impossible, it achieves a sustained and involving fairy-tale/fantasy atmosphere, in which it seems quite natural that people sing as much as they talk, or that a tussle over a stolen jacket should take on the form of a football game. Another memorable element is that Le Million includes what may be the funniest opera ever put on film (O that blonde-braided soprano! &quot;I laugh, ha! ha!&quot;) Also a delight is the casting: Clair has assembled a group of amazing, sharply different character actors, each of them illustrating with deadly satiric accuracy a bourgeois French &quot;type,&quot; so that the film seems like a set of Daumier prints come to life.&lt;br /&gt;&lt;br /&gt;The hilarity takes a little while to get rolling, and I found the characters not as emotionally engaging as they can be even in a light comedy (as they are, for instance, in many Lubitsch films.) For these reasons I refrained from giving it the highest rating. But these minor cavils shouldn&#39;t distract from an enthusiastic recommendation.&lt;br /&gt;&lt;br /&gt;Should you see it? By all means. Highly recommended whether you want a classic and influential work of cinema or just a fun comedy.  
pos   1260   Before I comment about this movie, you should realize that when I saw this movie, I expected the typical crap, horror, B-movie and just wanted to have fun. Jack Frost is one that not only delivers but is actually one of the best that I&#39;ve seen in a long time. Scott McDonald is great as Jack Frost, in fact I think he has a future in being psychopaths in big time movies if ever given the chance. McDonald is a serial killer who becomes a snowman through some stupid accidental mix of ridiculous elements. As soon as that snowman starts moving around and killing people, though, you will find it hard not to laugh. The lines that are said are completely retarded but really funny. The fact that the rest of the cast completely over-acts just adds to stupidity of the film, but it&#39;s stupidity is it&#39;s genius. The scene where the snowman is with the teenage girl is truly classic in B-movie, horror film fashion. I truly hope there is a sequel and I&#39;ll be right there to watch it on whatever cable channel does it. Of course it&#39;s only fun to watch the first few times and it&#39;s not exactly a good work of motion picture technology, but I just like to see snowmen kill people. I gave it a 7 out of 10, this is a great movie for dates and couples in the late hours.


PyTorchText BuketIterator

Batch size: 10

LABEL LENGTH TEXT
neg   1118   Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet&#39;s documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.&lt;br /&gt;&lt;br /&gt;The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.&lt;br /&gt;&lt;br /&gt;Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals.  
neg   1120   Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),&quot;Connie &amp; Carla&quot;,&#39;04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),&quot;Taking Lives&quot;,&#39;04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),&quot;Kinsey&quot;,&#39;04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),&quot;CSI-Vegas TV Series&quot;, plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain.  
pos   1120   I must say that, looking at Hamlet from the perspective of a student, Brannagh&#39;s version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.&lt;br /&gt;&lt;br /&gt;One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.&lt;br /&gt;&lt;br /&gt;Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it.  
pos   1120   As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While &quot;Angels in the Outfield&quot; fails miserably in the first category, it succeeds beautifully in the latter two. &quot;Angels&quot; weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father&#39;s promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise&#39;s downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup!  
neg   1121   I have a piece of advice for the people who made this movie too, if you&#39;re gonna make a movie like this be sure you got the f/x to back it up. Also don&#39;t get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don&#39;t blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let&#39;s talk about the acting for just a second, the Carradine guy who&#39;s career peaked in 1984 when he did &quot;Revenge of the Nerds&quot; (which was actually a great comedy). He&#39;s not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after &quot;Return of the Jedi&quot; and go quietly into the night. He made his mark as a &quot;Nerd&quot; and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I&#39;ll close on that note. Thank you for your time.  
neg   1121   By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That&#39;s the best thing that could be said about &quot;Adam Had Four Sons,&quot; a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter&#39;s four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it&#39;s a relief when Susan Hayward as the villainess enters midway through â€” she&#39;s about the only watchable person in the movie even though she&#39;s clearly channeling Bette Davis and Vivien Leigh; it&#39;s also the first in her long succession of alcoholic roles â€” but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted &quot;The Valley of Decision&quot; four years later.  
neg   1123   I have never read the book&quot;A wrinkle in time&quot;. To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn&#39;t be reviewing this movie i&#39;ll start off with that. Next i&#39;ll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than &quot;Merlin&quot;.&lt;br /&gt;&lt;br /&gt;How do I describe a TV movie? I have never written a review for one before. Well, i&#39;ll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. &lt;br /&gt;&lt;br /&gt;The film has cheesy special effects, a mildly interesting plot, scenes that make you go &quot;WTF&quot;. The movie is incredibly bad and it makes you go&quot;WTF&quot;. What did I expect? It&#39;s a TV movie. They usually aren&#39;t good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you&#39;ll most likely forget about it the second it&#39;s over. Well, maybe not the second it&#39;s over. But within a few minutes.&lt;br /&gt;&lt;br /&gt;A wrinkle in time:*/****  
neg   1123   After watching &quot;The Bodyguard&quot; last night, I felt compelled to write a review of it.&lt;br /&gt;&lt;br /&gt;This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.&lt;br /&gt;&lt;br /&gt;The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee&#39;s involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.&lt;br /&gt;&lt;br /&gt;The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I&#39;m sure this would have been a classic Kung Foo movie. As it is, it&#39;s more like another cheesy 70&#39;s action movie.&lt;br /&gt;&lt;br /&gt;Note: The opening sequence has a quote familiar to &quot;Pulp Fiction&quot; fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.&lt;br /&gt;&lt;br /&gt;Rating: 4 out of 10  
neg   1123   There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.&lt;br /&gt;&lt;br /&gt;Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by &quot;mayor&quot; Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren&#39;t bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth).  
neg   1124   The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).&lt;br /&gt;&lt;br /&gt;The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.&lt;br /&gt;&lt;br /&gt;This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :&lt;br /&gt;&lt;br /&gt;After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?&lt;br /&gt;&lt;br /&gt;Ajay Devgun is total miscast for portraying a rockstar.&lt;br /&gt;&lt;br /&gt;Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. &lt;br /&gt;&lt;br /&gt;Watch it maybe only for the last half hour.
</code></pre></div>
<h3 id="train-loop-examples_1"><strong>Train Loop Examples</strong><a class="headerlink" href="#train-loop-examples_1" title="Permanent link"></a></h3>
<p>Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset!</p>
<p>We see that we get same exact behavior as we did when using PyTorch Dataset. Now it depends on which way is easier for you to use PyTorchText BucketIterator: with PyTorch Dataset or with PyTorchText TabularDataset</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example of number of epochs.</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Example of loop through each epoch.</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

  <span class="c1"># Create batches - needs to be called before each loop.</span>
  <span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">create_batches</span><span class="p">()</span>

  <span class="c1"># Loop through BucketIterator.</span>
  <span class="k">for</span> <span class="n">sample_id</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">torchtext_train_dataloader</span><span class="o">.</span><span class="n">batches</span><span class="p">):</span>
    <span class="c1"># Put all example.text of batch in single array.</span>
    <span class="n">batch_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">example</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Batch examples lengths: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">%</span> <span class="nb">str</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">batch_text</span><span class="p">]))</span>

    <span class="c1"># Let&#39;s break early, you get the idea.</span>
    <span class="k">if</span> <span class="n">sample_id</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
      <span class="k">break</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Batch examples lengths: [791, 791, 792, 792, 793, 797, 797, 799, 799, 801]
Batch examples lengths: [4766, 4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313]
Batch examples lengths: [695, 695, 696, 696, 696, 697, 699, 699, 699, 700]
Batch examples lengths: [958, 959, 960, 961, 963, 963, 963, 966, 966, 967]
Batch examples lengths: [1200, 1203, 1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221]
Batch examples lengths: [2621, 2628, 2639, 2651, 2651, 2672, 2690, 2704, 2705, 2712]
Batch examples lengths: [1811, 1812, 1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878]
Batch examples lengths: [3104, 3107, 3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278]
Batch examples lengths: [3000, 3001, 3001, 3031, 3039, 3047, 3056, 3075, 3084, 3103]
Batch examples lengths: [1046, 1050, 1053, 1053, 1054, 1057, 1060, 1067, 1073, 1077]
Batch examples lengths: [749, 751, 751, 756, 758, 759, 760, 761, 762, 763]
</code></pre></div>
<h2 id="final-note"><strong>Final Note</strong><a class="headerlink" href="#final-note" title="Permanent link"></a></h2>
<p>If you made it this far <strong>Congrats!</strong> ðŸŽŠ and <strong>Thank you!</strong> ðŸ™ for your interest in my tutorial!</p>
<p>I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow.</p>
<p>Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials!</p>
<p>If you see something wrong please let me know by opening an issue on my <a href="https://github.com/gmihaila/ml_things/issues">ml_things GitHub repository</a>!</p>
<p>A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.</p>
<h2 id="contact-"><strong>Contact</strong> ðŸŽ£<a class="headerlink" href="#contact-" title="Permanent link"></a></h2>
<p>ðŸ¦Š GitHub: <a href="https://github.com/gmihaila">gmihaila</a></p>
<p>ðŸŒ Website: <a href="https://gmihaila.github.io/">gmihaila.github.io</a></p>
<p>ðŸ‘” LinkedIn: <a href="https://medium.com/r/?url=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fmihailageorge">mihailageorge</a></p>
<p>ðŸ“¬ Email: <a href="mailto:georgemihaila@my.unt.edu.com?subject=GitHub%20Website">georgemihaila@my.unt.edu.com</a></p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../bert_inner_workings/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Bert Inner Workings" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Bert Inner Workings
            </div>
          </div>
        </a>
      
      
        
        <a href="../pretrain_transformers_pytorch/" class="md-footer__link md-footer__link--next" aria-label="Next: Pretrain Transformers" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Pretrain Transformers
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2015 - 2020 <a href="https://github.com/gmihaila"  target="_blank" rel="noopener">George Mihaila</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://github.com/gmihaila" target="_blank" rel="noopener" title="gmihaila" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    <a href="https://gmihaila.medium.com/" target="_blank" rel="noopener" title="gmihaila" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271z"/></svg>
    </a>
  
    
    
    <a href="https://www.linkedin.com/in/mihailageorge" target="_blank" rel="noopener" title="mihailageorge" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["instant", "header.autohide", "search.highlight", "search.share", "search.suggest", "navigation.top", "navigation.expand"], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
    
  </body>
</html>